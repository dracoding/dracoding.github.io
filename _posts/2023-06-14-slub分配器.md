---
layout: post
title: 【memory】三、slub分配器
date: 2023-06-14
tags:  memory
---

## 内核常见的块内存分配器


| 块内存分配器 | 描述 |
| ---- | ---- |
| slab | 大内存时内存开销较大， 队列管理复杂   |
| slub |  |
| slob | 适用于小内存的嵌入式设备 |


## slub分配器

实现小块内存的分配， 内核实现了slab分配器

<div align="center">
<img src="/images/out/memory/slub.png">  
</div> 


## SLUB分配器的初始化

### kmem_cache的创建

kmem_cache_create -> kmem_cache_create_usercopy 


```
struct kmem_cache *
kmem_cache_create_usercopy(const char *name,
                  unsigned int size, unsigned int align,
                  slab_flags_t flags,
                  unsigned int useroffset, unsigned int usersize,
                  void (*ctor)(void *))
{
        struct kmem_cache *s = NULL;
        const char *cache_name;
        int err;

        mutex_lock(&slab_mutex);

        err = kmem_cache_sanity_check(name, size);
        if (err) {
                goto out_unlock;
        }

        /* Refuse requests with allocator specific flags */
        if (flags & ~SLAB_FLAGS_PERMITTED) {
                err = -EINVAL;
                goto out_unlock;
        }

        flags &= CACHE_CREATE_MASK;

        /* Fail closed on bad usersize of useroffset values. */
        if (WARN_ON(!usersize && useroffset) ||
            WARN_ON(size < usersize || size - usersize < useroffset))
                usersize = useroffset = 0;

        if (!usersize)
                s = __kmem_cache_alias(name, size, align, flags, ctor);   // 从缓存查找
        if (s)
                goto out_unlock;

        cache_name = kstrdup_const(name, GFP_KERNEL);
        if (!cache_name) {
                err = -ENOMEM;
                goto out_unlock;
        }

        s = create_cache(cache_name, size,
                         calculate_alignment(flags, align, size),
                         flags, useroffset, usersize, ctor, NULL);
        if (IS_ERR(s)) {
                err = PTR_ERR(s);
                kfree_const(cache_name);
        }

out_unlock:
        mutex_unlock(&slab_mutex);

        if (err) {
                if (flags & SLAB_PANIC)
                        panic("kmem_cache_create: Failed to create slab '%s'. Error %d\n",
                                name, err);
                else {
                        pr_warn("kmem_cache_create(%s) failed with error %d\n",
                                name, err);
                        dump_stack();
                }
                return NULL;
        }
        return s;
}
```

create_cache创建缓存

```
static struct kmem_cache *create_cache(const char *name,
                unsigned int object_size, unsigned int align,
                slab_flags_t flags, unsigned int useroffset,
                unsigned int usersize, void (*ctor)(void *),
                struct kmem_cache *root_cache)
{       
        struct kmem_cache *s;
        int err;

        if (WARN_ON(useroffset + usersize > object_size))
                useroffset = usersize = 0;
                         
        err = -ENOMEM;   
        s = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);
        if (!s)
                goto out;

        s->name = name;
        s->size = s->object_size = object_size;
        s->align = align;
        s->ctor = ctor;
        s->useroffset = useroffset;
        s->usersize = usersize;

        err = __kmem_cache_create(s, flags);
        if (err)
                goto out_free_cache;

        s->refcount = 1;
        list_add(&s->list, &slab_caches);
out:
        if (err)
                return ERR_PTR(err);
        return s;

out_free_cache:
        kmem_cache_free(kmem_cache, s);
        goto out;
}
```


```
int __kmem_cache_create(struct kmem_cache *s, slab_flags_t flags)
{
        int err;

        err = kmem_cache_open(s, flags);
        if (err)
                return err;

        /* Mutex is not taken during early boot */
        if (slab_state <= UP)
                return 0;

        err = sysfs_slab_add(s);
        if (err)
                __kmem_cache_release(s);

        return err;
}
```

kmem_cache_open

```
static int kmem_cache_open(struct kmem_cache *s, slab_flags_t flags)
{
        s->flags = kmem_cache_flags(s->size, flags, s->name);

        if (!calculate_sizes(s, -1))
                goto error;
        if (disable_higher_order_debug) {
                /*
                 * Disable debugging flags that store metadata if the min slab
                 * order increased.
                 */
                if (get_order(s->size) > get_order(s->object_size)) {
                        s->flags &= ~DEBUG_METADATA_FLAGS;
                        s->offset = 0;
                        if (!calculate_sizes(s, -1))
                                goto error;
                }
        }


        /*
         * The larger the object size is, the more pages we want on the partial
         * list to avoid pounding the page allocator excessively.
         */
        set_min_partial(s, ilog2(s->size) / 2);

        set_cpu_partial(s);

#ifdef CONFIG_NUMA
        s->remote_node_defrag_ratio = 1000;
#endif

        /* Initialize the pre-computed randomized freelist if slab is up */
        if (slab_state >= UP) {
                if (init_cache_random_seq(s))
                        goto error;
        }
        if (!init_kmem_cache_nodes(s))     // 初始化kmem_cache_node
                goto error;

        if (alloc_kmem_cache_cpus(s))    // 初始化cpu_slab
                return 0;

error:
        __kmem_cache_release(s);
        return -EINVAL;
}
```

## 申请对象

kmalloc -> __kmalloc -> 

```
void *__kmalloc(size_t size, gfp_t flags)
{
        struct kmem_cache *s;
        void *ret;

        if (unlikely(size > KMALLOC_MAX_CACHE_SIZE))
                return kmalloc_large(size, flags);

        s = kmalloc_slab(size, flags);    // 冲缓存获取

        if (unlikely(ZERO_OR_NULL_PTR(s)))
                return s;

        ret = slab_alloc(s, flags, _RET_IP_, size);

        trace_kmalloc(_RET_IP_, ret, size, s->size, flags);

        ret = kasan_kmalloc(s, ret, size, flags);

        return ret;     
}

```

slab_alloc -> slab_alloc_node

```
static __always_inline void *slab_alloc_node(struct kmem_cache *s,
                gfp_t gfpflags, int node, unsigned long addr, size_t orig_size)
{       
        void *object;
        struct kmem_cache_cpu *c;
        struct page *page;
        unsigned long tid;
        struct obj_cgroup *objcg = NULL;

        s = slab_pre_alloc_hook(s, &objcg, 1, gfpflags);
        if (!s)
                return NULL;

        object = kfence_alloc(s, orig_size, gfpflags);
        if (unlikely(object))
                goto out;

redo:
        /*
         * Must read kmem_cache cpu data via this cpu ptr. Preemption is
         * enabled. We may switch back and forth between cpus while
         * reading from one cpu area. That does not matter as long
         * as we end up on the original cpu again when doing the cmpxchg.
         *
         * We should guarantee that tid and kmem_cache are retrieved on
         * the same cpu. It could be different if CONFIG_PREEMPTION so we need
         * to check if it is matched or not.
         */
        do {
                tid = this_cpu_read(s->cpu_slab->tid);
                c = raw_cpu_ptr(s->cpu_slab);
        } while (IS_ENABLED(CONFIG_PREEMPTION) &&
                 unlikely(tid != READ_ONCE(c->tid)));

        /*
         * Irqless object alloc/free algorithm used here depends on sequence
         * of fetching cpu_slab's data. tid should be fetched before anything
         * on c to guarantee that object and page associated with previous tid
         * won't be used with current tid. If we fetch tid first, object and
         * page could be one associated with next tid and our alloc/free
         * request will be failed. In this case, we will retry. So, no problem.
         */
        barrier();
        /*
         * The transaction ids are globally unique per cpu and per operation on
         * a per cpu queue. Thus they can be guarantee that the cmpxchg_double
         * occurs on the right processor and that there was no operation on the
         * linked list in between.
         */

        object = c->freelist;
        page = c->page;
        if (unlikely(!object || !page || !node_match(page, node))) {
                object = __slab_alloc(s, gfpflags, node, addr, c);
        } else {
                void *next_object = get_freepointer_safe(s, object);

                /*
                 * The cmpxchg will only match if there was no additional
                 * operation and if we are on the right processor.
                 *
                 * The cmpxchg does the following atomically (without lock
                 * semantics!)
                 * 1. Relocate first pointer to the current per cpu area.
                 * 2. Verify that tid and freelist have not been changed
                 * 3. If they were not changed replace tid and freelist
                 *
                 * Since this is without lock semantics the protection is only
                 * against code executing on this cpu *not* from access by
                 * other cpus.
                 */
                if (unlikely(!this_cpu_cmpxchg_double(
                                s->cpu_slab->freelist, s->cpu_slab->tid,
                                object, tid,
                                next_object, next_tid(tid)))) {

                        note_cmpxchg_failure("slab_alloc", s, tid);
                        goto redo;
                }
                prefetch_freepointer(s, next_object);
                stat(s, ALLOC_FASTPATH);
        }

        maybe_wipe_obj_freeptr(s, object);

        if (unlikely(slab_want_init_on_alloc(gfpflags, s)) && object)
                memset(object, 0, s->object_size);

out:
        slab_post_alloc_hook(s, objcg, gfpflags, 1, &object);

        return object;
}
```

__slab_alloc -> ___slab_alloc


```
static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
                          unsigned long addr, struct kmem_cache_cpu *c)
{
        void *freelist;
        struct page *page;

        stat(s, ALLOC_SLOWPATH);

        page = c->page;
        if (!page) {
                /*
                 * if the node is not online or has no normal memory, just
                 * ignore the node constraint
                 */
                if (unlikely(node != NUMA_NO_NODE &&
                             !node_isset(node, slab_nodes)))
                        node = NUMA_NO_NODE;
                goto new_slab;
        }
redo:

        if (unlikely(!node_match(page, node))) {
                /*
                 * same as above but node_match() being false already
                 * implies node != NUMA_NO_NODE
                 */
                if (!node_isset(node, slab_nodes)) {
                        node = NUMA_NO_NODE;
                        goto redo;
                } else {
                        stat(s, ALLOC_NODE_MISMATCH);
                        deactivate_slab(s, page, c->freelist, c);
                        goto new_slab;
                }
        }

        /*
         * By rights, we should be searching for a slab page that was
         * PFMEMALLOC but right now, we are losing the pfmemalloc
         * information when the page leaves the per-cpu allocator
         */
        if (unlikely(!pfmemalloc_match(page, gfpflags))) {
                deactivate_slab(s, page, c->freelist, c);     // 从本地缓存放回node
                goto new_slab;
        }
        /* must check again c->freelist in case of cpu migration or IRQ */
        freelist = c->freelist;
        if (freelist)
                goto load_freelist;

        freelist = get_freelist(s, page);

        if (!freelist) {
                c->page = NULL;
                c->tid = next_tid(c->tid);
                stat(s, DEACTIVATE_BYPASS);
                goto new_slab;
        }

        stat(s, ALLOC_REFILL);

load_freelist:
        /*
         * freelist is pointing to the list of objects to be used.
         * page is pointing to the page from which the objects are obtained.
         * That page must be frozen for per cpu allocations to work.
         */
        VM_BUG_ON(!c->page->frozen);
        c->freelist = get_freepointer(s, freelist);     // 从本地链表获取
        c->tid = next_tid(c->tid);
        return freelist;

new_slab:
        if (slub_percpu_partial(c)) {
                page = c->page = slub_percpu_partial(c);
                slub_set_percpu_partial(c, page);
                stat(s, CPU_PARTIAL_ALLOC);
                goto redo;
        }

        freelist = new_slab_objects(s, gfpflags, node, &c);   // 创建新的slab

        if (unlikely(!freelist)) {
                slab_out_of_memory(s, gfpflags, node);
                return NULL;
        }

        page = c->page;
        if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
                goto load_freelist;

        /* Only entered in the debug case */
        if (kmem_cache_debug(s) &&
                        !alloc_debug_processing(s, page, freelist, addr))
                goto new_slab;  /* Slab failed checks. Next slab needed */

        deactivate_slab(s, page, get_freepointer(s, freelist), c);
        return freelist;
}
```

<br>
转载请注明： [【memory】三、slub分配器](https://dracoding.github.io/2023/06/slub分配器/) 

---
layout: post
title: 【memory】二、伙伴分配器
date: 2023-06-12
tags:  memory
---

## 伙伴分配器

伙伴系统进行物理页面的分配，用于分配连续的物理页面， 分配的页面个数只能是2的次幂。


<div align="center">
<img src="/images/out/memory/mm_page.png">  
</div> 


## 常用的函数

1、alloc_pages(gfp_mask, order), 请求分配一个2^order的页块。 

2、alloc_page(gfp_mask), 请求分配一个页块。 


## 分配标志位

| 优先级 | 标记位 | 描述 |
| ---- | ----| ---- |
| 区域修饰 | __GFP_DMA | DMA区域分配页  |
| 区域修饰 | __GFP_HIGHMEM | 高端内存区域分配页 |
| 区域修饰 | __GFP_DMA32 | DMA32区域分配页 |
| 区域修饰 | __GFP_MOVABLE | 可移动区域分配页 |
| 页移动性和位置 | __GFP_RECLAIMABLE | 申请可回收页 |
| 页移动性和位置 | __GFP_WRITE | 打算写物理页 |
| 页移动性和位置 | __GFP_HARDWALL | 实施cpuset分配策略 |
| 页移动性和位置 | __GFP_THISNODE | 从指定节点分配页 |
| 页移动性和位置 | __GFP_ACCOUNT | 分配的页记账到内存控制组 |
| 页移动性和位置 | __GFP_RELIABLE | 从mirror区域分配内存 |
| 水线修饰符 | __GFP_ATOMIC | 指明调用者时高优先级的， 不能回收页或睡眠 |
| 水线修饰符 | __GFP_HIGH | 指明调用者时高优先级的 |
| 水线修饰符 | __GFP_MEMALLOC | 允许访问所有内存 |
| 水线修饰符 | __GFP_NOMEMALLOC | 禁止访问紧急保留内存， 优先级高 |
| 回收修饰符 | __GFP_IO | 允许读写存储设备 |
| 回收修饰符 | __GFP_FS | 允许向下调到文件系统 |
| 回收修饰符 | __GFP_DIRECT_RECLAIM | 调用者直接回收页 |
| 回收修饰符 | __GFP_KSWAPD_RECLAIM | 空闲页数达到低水线时， 唤醒回收线程 |
| 回收修饰符 | __GFP_RECLAIM | 允许直接回收页和异步回收页 |
| 回收修饰符 | __GFP_RETRY_MAYFAIL | 允许重试指定的次数 |
| 回收修饰符 | __GFP_NOFAIL | 无限次重试，不能失败 |
| 回收修饰符 | __GFP_NORETRY | 分配失败不重试 |
| 行动修饰符 | __GFP_NOWARN | 分配失败， 不打印警告信息 |
| 行动修饰符 | __GFP_COMP | 分配的页块组成复合页 |
| 行动修饰符 | __GFP_ZERO | 页用0初始化 |


## 以alloc_pages为例

alloc_pages -> alloc_pages_node -> __alloc_pages_node -> __alloc_pages

```
struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
                                                        nodemask_t *nodemask)
{
        struct page *page;
        unsigned int alloc_flags = ALLOC_WMARK_LOW; 
        gfp_t alloc_gfp; /* The gfp_t that was actually used for allocation */
        struct alloc_context ac = { };

        trace_mm_page_alloc_enter(order, gfp);

        /*
         * There are several places where we assume that the order value is sane
         * so bail out early if the request is out of bound.
         */
        if (unlikely(order >= MAX_ORDER)) {
                WARN_ON_ONCE(!(gfp & __GFP_NOWARN));
                return NULL;
        }

        gfp &= gfp_allowed_mask;

        prepare_before_alloc(&gfp);

retry: 
        alloc_gfp = gfp;
        if (!prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac,  // 初始化ac
                        &alloc_gfp, &alloc_flags))
                return NULL;

        /*
         * Forbid the first pass from falling back to types that fragment
         * memory until all local zones are considered.
         */
        alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp);

        /* Before alloc from buddy system, alloc from hpool firstly */
        page = alloc_page_from_dhugetlb_pool(alloc_gfp, order, alloc_flags);   // 优先从动态大页分配， 默认不使能
        if (page)
                goto out;

        /* First allocation attempt */
        page = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);   // 优先从freelist分配物理页面
        if (likely(page))
                goto out;
        /*
         * Apply scoped allocation constraints. This is mainly about GFP_NOFS
         * resp. GFP_NOIO which has to be inherited for all allocation requests
         * from a particular context which has been marked by
         * memalloc_no{fs,io}_{save,restore}.
         */
        alloc_gfp = current_gfp_context(gfp);
        ac.spread_dirty_pages = false;

        /*
         * Restore the original nodemask if it was potentially replaced with
         * &cpuset_current_mems_allowed to optimize the fast-path attempt.
         */
        ac.nodemask = nodemask;

        page = __alloc_pages_slowpath(alloc_gfp, order, &ac);

out:
        if (memcg_kmem_enabled() && (gfp & __GFP_ACCOUNT) && page &&
            unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {
                __free_pages(page, order);
                page = NULL;
        }

        if (check_after_alloc(&gfp, order, preferred_nid, &ac, &page))
                goto retry;

        trace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);

        return page;
}
```

### get_page_from_freelist

```
static struct page *
get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
                                                const struct alloc_context *ac)
{
        struct zoneref *z;
        struct zone *zone;
        struct pglist_data *last_pgdat_dirty_limit = NULL;
        bool no_fallback;

retry:
        /*
         * Scan zonelist, looking for a zone with enough free.
         * See also __cpuset_node_allowed() comment in kernel/cpuset.c.
         */
        no_fallback = alloc_flags & ALLOC_NOFRAGMENT;
        z = ac->preferred_zoneref;
        for_next_zone_zonelist_nodemask(zone, z, ac->highest_zoneidx,
                                        ac->nodemask) {
                struct page *page;
                unsigned long mark;

                /* skip non-movable zone for normal user tasks */
                if (skip_none_movable_zone(gfp_mask, z))
                        continue;

                /*
                 * CDM nodes get skipped if the requested gfp flag
                 * does not have __GFP_THISNODE set or the nodemask
                 * does not have any CDM nodes in case the nodemask
                 * is non NULL (explicit allocation requests from
                 * kernel or user process MPOL_BIND policy which has
                 * CDM nodes).
                 */
                if (is_cdm_node(zone->zone_pgdat->node_id)) {
                        if (!(gfp_mask & __GFP_THISNODE)) {
                                if (!ac->nodemask)
                                        continue;
                        }
                }
                if (cpusets_enabled() &&
                        (alloc_flags & ALLOC_CPUSET) &&
                        !__cpuset_zone_allowed(zone, gfp_mask)
#ifdef CONFIG_COHERENT_DEVICE
                        && !(alloc_flags & ALLOC_CDM)
#endif
                )
                                continue;
                /*
                 * When allocating a page cache page for writing, we
                 * want to get it from a node that is within its dirty
                 * limit, such that no single node holds more than its
                 * proportional share of globally allowed dirty pages.
                 * The dirty limits take into account the node's
                 * lowmem reserves and high watermark so that kswapd
                 * should be able to balance it without having to
                 * write pages from its LRU list.
                 *
                 * XXX: For now, allow allocations to potentially
                 * exceed the per-node dirty limit in the slowpath
                 * (spread_dirty_pages unset) before going into reclaim,
                 * which is important when on a NUMA setup the allowed
                 * nodes are together not big enough to reach the
                 * global limit.  The proper fix for these situations
                 * will require awareness of nodes in the
                 * dirty-throttling and the flusher threads.
                 */
                if (ac->spread_dirty_pages) {
                        if (last_pgdat_dirty_limit == zone->zone_pgdat)
                                continue;

                        if (!node_dirty_ok(zone->zone_pgdat)) {        // 如果脏页超过了限制， 则跳过
                                last_pgdat_dirty_limit = zone->zone_pgdat;
                                continue;
                        }
                }
                if (no_fallback && nr_online_nodes > 1 &&
                    zone != ac->preferred_zoneref->zone) {
                        int local_nid;

                        /*
                         * If moving to a remote node, retry but allow
                         * fragmenting fallbacks. Locality is more important
                         * than fragmentation avoidance.
                         */
                        local_nid = zone_to_nid(ac->preferred_zoneref->zone);
                        if (zone_to_nid(zone) != local_nid) {
                                alloc_flags &= ~ALLOC_NOFRAGMENT;
                                goto retry;
                        }
                }

                mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);   // 获取低水位线
                if (!zone_watermark_fast(zone, order, mark,                 // 判断水位是否满足
                                       ac->highest_zoneidx, alloc_flags,
                                       gfp_mask)) {
                        int ret;

                        /* Checked here to keep the fast path fast */
                        BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
                        if (alloc_flags & ALLOC_NO_WATERMARKS)
                                goto try_this_zone;

                        if (node_reclaim_mode == 0 ||
                            !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))  // 没有开启内存回收功能
                                continue;
                        ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);   // 进行页面的回收
                        switch (ret) {
                        case NODE_RECLAIM_NOSCAN:
                                /* did not scan */
                                continue;
                        case NODE_RECLAIM_FULL:
                                /* scanned but unreclaimable */
                                continue;
                        default:
                                /* did we reclaim enough */
                                if (zone_watermark_ok(zone, order, mark,   // 回收之后重新检查水线
                                        ac->highest_zoneidx, alloc_flags))
                                        goto try_this_zone;

                                continue;
                        }
                }

try_this_zone:
                page = rmqueue(ac->preferred_zoneref->zone, zone, order,   // 从当前区域分配页
                                gfp_mask, alloc_flags, ac->migratetype);
                if (page) {
                        prep_new_page(page, order, gfp_mask, alloc_flags);

                        /*
                         * If this is a high-order atomic allocation then check
                         * if the pageblock should be reserved for the future
                         */
                        if (unlikely(order && (alloc_flags & ALLOC_HARDER)))
                                reserve_highatomic_pageblock(page, zone, order);

                        return page;
                } else {
#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
                        /* Try again if zone has deferred pages */
                        if (static_branch_unlikely(&deferred_pages)) {
                                if (_deferred_grow_zone(zone, order))
                                        goto try_this_zone;
                        }
#endif
                }
        }
        /*
         * It's possible on a UMA machine to get through all zones that are
         * fragmented. If avoiding fragmentation, reset and try again.
         */
        if (no_fallback) {
                alloc_flags &= ~ALLOC_NOFRAGMENT;
                goto retry;
        }

        return NULL;
}
```

for_next_zone_zonelist_nodemask

```
#define for_next_zone_zonelist_nodemask(zone, z, highidx, nodemask) \   // 遍历至highidx的所有zone
        for (zone = z->zone;    \
                zone;                                                   \
                z = next_zones_zonelist(++z, highidx, nodemask),        \
                        zone = zonelist_zone(z))

```

rmqueue从指定的zone中分配物理页面

```
static inline
struct page *rmqueue(struct zone *preferred_zone,
                        struct zone *zone, unsigned int order,
                        gfp_t gfp_flags, unsigned int alloc_flags,
                        int migratetype)
{
        unsigned long flags;
        struct page *page;

        if (likely(order == 0)) {           // 分配1页
                /*
                 * MIGRATE_MOVABLE pcplist could have the pages on CMA area and
                 * we need to skip it when CMA area isn't allowed.
                 */
                if (!IS_ENABLED(CONFIG_CMA) || alloc_flags & ALLOC_CMA ||
                                migratetype != MIGRATE_MOVABLE) {
                        page = rmqueue_pcplist(preferred_zone, zone, gfp_flags,
                                        migratetype, alloc_flags);
                        goto out;
                }
        }

        /*
         * We most definitely don't want callers attempting to
         * allocate greater than order-1 page units with __GFP_NOFAIL.
         */
        WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));
        spin_lock_irqsave(&zone->lock, flags);
        do {
                page = NULL;
                /*
                 * order-0 request can reach here when the pcplist is skipped
                 * due to non-CMA allocation context. HIGHATOMIC area is
                 * reserved for high-order atomic allocation, so order-0
                 * request should skip it.
                 */
                if (order > 0 && alloc_flags & ALLOC_HARDER) {
                        page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
                        if (page)
                                trace_mm_page_alloc_zone_locked(page, order, migratetype);
                }
                if (!page)
                        page = __rmqueue(zone, order, migratetype, alloc_flags);
        } while (page && check_new_pages(page, order));
        spin_unlock(&zone->lock);
        if (!page)
                goto failed;
        __mod_zone_freepage_state(zone, -(1 << order),
                                  get_pcppage_migratetype(page));

        __count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
        zone_statistics(preferred_zone, zone);
        local_irq_restore(flags);

out:
        /* Separate test+clear to avoid unnecessary atomics */
        if (test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags)) {
                clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
                wakeup_kswapd(zone, 0, 0, zone_idx(zone));
        }

        VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
        return page;

failed:
        local_irq_restore(flags);
        return NULL;
}
```

分配1页， rmqueue_pcplist从每处理器的集合分配页

```
/* Lock and remove page from the per-cpu list */
static struct page *rmqueue_pcplist(struct zone *preferred_zone,
                        struct zone *zone, gfp_t gfp_flags,
                        int migratetype, unsigned int alloc_flags)
{
        struct per_cpu_pages *pcp;
        struct list_head *list;
        struct page *page;
        unsigned long flags;

        local_irq_save(flags);
        pcp = &this_cpu_ptr(zone->pageset)->pcp;
        list = &pcp->lists[migratetype];
        page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);
        if (page) {
                __count_zid_vm_events(PGALLOC, page_zonenum(page), 1);
                zone_statistics(preferred_zone, zone);
        }
        local_irq_restore(flags);
        return page;
}

static inline
struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
                        unsigned int alloc_flags,
                        struct per_cpu_pages *pcp,
                        struct list_head *list)
{
        struct page *page;

        do {
                if (list_empty(list)) {  // 如果list为空， 批量申请加入链表
                        pcp->count += rmqueue_bulk(zone, 0,
                                        READ_ONCE(pcp->batch), list,
                                        migratetype, alloc_flags);
                        if (unlikely(list_empty(list)))
                                return NULL;
                }

                page = list_first_entry(list, struct page, lru);
                list_del(&page->lru);
                pcp->count--;
        } while (check_new_pcp(page));

        return page;
}
```

__rmqueue_smallest， 从申请阶数到最大阶数


```
static __always_inline
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
                                                int migratetype)
{
        unsigned int current_order;
        struct free_area *area;
        struct page *page;

        /* Find a page of the appropriate size in the preferred list */
        for (current_order = order; current_order < MAX_ORDER; ++current_order) {
                area = &(zone->free_area[current_order]);
                page = get_page_from_free_area(area, migratetype);
                if (!page)
                        continue;
                del_page_from_free_list(page, zone, current_order);
                expand(zone, page, order, current_order, migratetype);
                set_pcppage_migratetype(page, migratetype);
                return page;
        }

        return NULL;
}


```


```
static inline void expand(struct zone *zone, struct page *page, 
        int low, int high, int migratetype)
{
        unsigned long size = 1 << high;

        while (high > low) {
                high--;
                size >>= 1;
                VM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);

                /*
                 * Mark as guard pages (or page), that will allow to
                 * merge back to allocator when buddy will be freed.
                 * Corresponding page table entries will not be touched,
                 * pages will stay not present in virtual address space
                 */
                if (set_page_guard(zone, &page[size], high, migratetype))
                        continue;

                add_to_free_list(&page[size], zone, high, migratetype);
                set_buddy_order(&page[size], high);
        }
}

```

### __alloc_pages_slowpath慢速路径

```
static inline struct page *
__alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
                                                struct alloc_context *ac)
{
        bool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;
        const bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;
        struct page *page = NULL;
        unsigned int alloc_flags;
        unsigned long did_some_progress;
        enum compact_priority compact_priority;
        enum compact_result compact_result;
        int compaction_retries;
        int no_progress_loops;
        unsigned int cpuset_mems_cookie;
        int reserve_flags;

        /*
         * We also sanity check to catch abuse of atomic reserves being used by
         * callers that are not in atomic context.
         */
        if (WARN_ON_ONCE((gfp_mask & (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==
                                (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))
                gfp_mask &= ~__GFP_ATOMIC;

retry_cpuset:
        compaction_retries = 0;
        no_progress_loops = 0;
        compact_priority = DEF_COMPACT_PRIORITY;
        cpuset_mems_cookie = read_mems_allowed_begin();

        /*
         * The fast path uses conservative alloc_flags to succeed only until
         * kswapd needs to be woken up, and to avoid the cost of setting up
         * alloc_flags precisely. So we do that now.
         */
        alloc_flags = gfp_to_alloc_flags(gfp_mask);

        /*
         * We need to recalculate the starting point for the zonelist iterator
         * because we might have used different nodemask in the fast path, or
         * there was a cpuset modification and we are retrying - otherwise we
         * could end up iterating over non-eligible zones endlessly.
         */
        ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
                                        ac->highest_zoneidx, ac->nodemask);
        if (!ac->preferred_zoneref->zone)
                goto nopage;
        if (alloc_flags & ALLOC_KSWAPD)
                wake_all_kswapds(order, gfp_mask, ac);  // 唤醒异步回收线程

        mem_reliable_fallback_slowpath(gfp_mask, ac);

        /*
         * The adjusted alloc_flags might result in immediate success, so try
         * that first
         */
        page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
        if (page)
                goto got_pg;

        /*
         * For costly allocations, try direct compaction first, as it's likely
         * that we have enough base pages and don't need to reclaim. For non-
         * movable high-order allocations, do that as well, as compaction will
         * try prevent permanent fragmentation by migrating from blocks of the
         * same migratetype.
         * Don't try this for allocations that are allowed to ignore
         * watermarks, as the ALLOC_NO_WATERMARKS attempt didn't yet happen.
         */
        if (can_direct_reclaim &&
                        (costly_order ||
                           (order > 0 && ac->migratetype != MIGRATE_MOVABLE))
                        && !gfp_pfmemalloc_allowed(gfp_mask)) {
                page = __alloc_pages_direct_compact(gfp_mask, order,     // 执行异步的内存碎片整理
                                                alloc_flags, ac,
                                                INIT_COMPACT_PRIORITY,
                                                &compact_result);
                if (page)
                        goto got_pg;
                /*
                 * Checks for costly allocations with __GFP_NORETRY, which
                 * includes some THP page fault allocations
                 */
                if (costly_order && (gfp_mask & __GFP_NORETRY)) {
                        /*
                         * If allocating entire pageblock(s) and compaction
                         * failed because all zones are below low watermarks
                         * or is prohibited because it recently failed at this
                         * order, fail immediately unless the allocator has
                         * requested compaction and reclaim retry.
                         *
                         * Reclaim is
                         *  - potentially very expensive because zones are far
                         *    below their low watermarks or this is part of very
                         *    bursty high order allocations,
                         *  - not guaranteed to help because isolate_freepages()
                         *    may not iterate over freed pages as part of its
                         *    linear scan, and
                         *  - unlikely to make entire pageblocks free on its
                         *    own.
                         */
                        if (compact_result == COMPACT_SKIPPED ||
                            compact_result == COMPACT_DEFERRED)
                                goto nopage;

                        /*
                         * Looks like reclaim/compaction is worth trying, but
                         * sync compaction could be very expensive, so keep
                         * using async compaction.
                         */
                        compact_priority = INIT_COMPACT_PRIORITY;
                }
        }

retry:
        /* Ensure kswapd doesn't accidentally go to sleep as long as we loop */
        if (alloc_flags & ALLOC_KSWAPD)
                wake_all_kswapds(order, gfp_mask, ac);

        reserve_flags = __gfp_pfmemalloc_flags(gfp_mask);
        if (reserve_flags)
                alloc_flags = current_alloc_flags(gfp_mask, reserve_flags);

        /*
         * Reset the nodemask and zonelist iterators if memory policies can be
         * ignored. These allocations are high priority and system rather than
         * user oriented.
         */
        if (!(alloc_flags & ALLOC_CPUSET) || reserve_flags) {
                ac->nodemask = NULL;
                ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
                                        ac->highest_zoneidx, ac->nodemask);
        }

        /* Attempt with potentially adjusted zonelist and alloc_flags */
        page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
        if (page)
                goto got_pg;

        /* Caller is not willing to reclaim, we can't balance anything */
        if (!can_direct_reclaim)
                goto nopage;

        /* Avoid recursion of direct reclaim */
        if (current->flags & PF_MEMALLOC)
                goto nopage;

        /* Try direct reclaim and then allocating */
        page = __alloc_pages_direct_reclaim(gfp_mask, order, alloc_flags, ac,      // 直接回收页
                                                        &did_some_progress);
        if (page)
                goto got_pg;

        /* Try direct compaction and then allocating */
        page = __alloc_pages_direct_compact(gfp_mask, order, alloc_flags, ac,
                                        compact_priority, &compact_result);
        if (page)
                goto got_pg;

        /* Do not loop if specifically requested */
        if (gfp_mask & __GFP_NORETRY)
                goto nopage;

        /*
         * Do not retry costly high order allocations unless they are
         * __GFP_RETRY_MAYFAIL
         */
        if (costly_order && !(gfp_mask & __GFP_RETRY_MAYFAIL))
                goto nopage;

        if (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,  // 是否重试回收页
                                 did_some_progress > 0, &no_progress_loops))
                goto retry;

        /*
         * It doesn't make any sense to retry for the compaction if the order-0
         * reclaim is not able to make any progress because the current
         * implementation of the compaction depends on the sufficient amount
         * of free memory (see __compaction_suitable)
         */
        if (did_some_progress > 0 &&
                        should_compact_retry(ac, order, alloc_flags,
                                compact_result, &compact_priority,
                                &compaction_retries))
                goto retry;


        /* Deal with possible cpuset update races before we start OOM killing */
        if (check_retry_cpuset(cpuset_mems_cookie, ac))
                goto retry_cpuset;

        /* Reclaim has failed us, start killing things */
        page = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);  // OOM killer
        if (page)
                goto got_pg;

        /* Avoid allocations with no watermarks from looping endlessly */
        if (tsk_is_oom_victim(current) &&
            (alloc_flags & ALLOC_OOM ||
             (gfp_mask & __GFP_NOMEMALLOC)))
                goto nopage;
        /* Retry as long as the OOM killer is making progress */
        if (did_some_progress) {
                no_progress_loops = 0;
                goto retry;
        }

nopage:
        /* Deal with possible cpuset update races before we fail */
        if (check_retry_cpuset(cpuset_mems_cookie, ac))
                goto retry_cpuset;

        /*
         * Make sure that __GFP_NOFAIL request doesn't leak out and make sure
         * we always retry
         */
        if (gfp_mask & __GFP_NOFAIL) {
                /*
                 * All existing users of the __GFP_NOFAIL are blockable, so warn
                 * of any new users that actually require GFP_NOWAIT
                 */
                if (WARN_ON_ONCE(!can_direct_reclaim))
                        goto fail;

                /*
                 * PF_MEMALLOC request from this context is rather bizarre
                 * because we cannot reclaim anything and only can loop waiting
                 * for somebody to do a work for us
                 */
                WARN_ON_ONCE(current->flags & PF_MEMALLOC);

                /*
                 * non failing costly orders are a hard requirement which we
                 * are not prepared for much so let's warn about these users
                 * so that we can identify them and convert them to something
                 * else.
                 */
                WARN_ON_ONCE(order > PAGE_ALLOC_COSTLY_ORDER);

                /*
                 * Help non-failing allocations by giving them access to memory
                 * reserves but do not use ALLOC_NO_WATERMARKS because this
                 * could deplete whole memory reserves which would just make
                 * the situation worse
                 */
                page = __alloc_pages_cpuset_fallback(gfp_mask, order, ALLOC_HARDER, ac);
                if (page)
                        goto got_pg;

                cond_resched();
                goto retry;
        }
fail:
        warn_alloc(gfp_mask, ac->nodemask,
                        "page allocation failure: order:%u", order);
got_pg:
        return page;
}
```

<br>
转载请注明： [【memory】二、伙伴系统](https://dracoding.github.io/2023/06/伙伴系统/) 

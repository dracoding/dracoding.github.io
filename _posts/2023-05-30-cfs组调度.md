---
layout: post
title: 【sched】三、CFS组调度
date: 2023-05-30
tags:  sched 
---

## CFS组调度相关的结构体

组调度被cgroup的cpu子系统使用， cfs组调度相关的结构体见第一章的task_cgroup。

## 调度组的创建

setsid() -> ksys_setsid() -> sched_autogroup_create_attach() -> autogroup_create() -> sched_create_group()

cpu_cgrp_subsys->css_alloc() -> cpu_cgroup_css_alloc() -> sched_create_group()


```
struct task_group *sched_create_group(struct task_group *parent)
{
        struct task_group *tg;

        tg = kmem_cache_alloc(task_group_cache, GFP_KERNEL | __GFP_ZERO);   // 分配一个数据结构实例。
        if (!tg)
                return ERR_PTR(-ENOMEM);

        if (!alloc_fair_sched_group(tg, parent))        // 分配cfs调度需要的数据结构， cfs_rq队列和对应的调度实体
                goto err;

#ifdef CONFIG_QOS_SCHED
        if (!alloc_qos_sched_group(tg, parent))
                goto err;
#endif

        if (!alloc_rt_sched_group(tg, parent))
                goto err;

#ifdef CONFIG_BPF_SCHED
        tg_init_tag(tg, parent);
#endif

        alloc_uclamp_sched_group(tg, parent);

        return tg;

err:
        sched_free_group(tg);
        return ERR_PTR(-ENOMEM);
}

```

alloc_fair_sched_group函数


```
int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)
{
        struct sched_entity *se;
        struct cfs_rq *cfs_rq;
        int i;

        tg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);  // 指针数组， 数组长度为cpu的个数
        if (!tg->cfs_rq)
                goto err;
        tg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);
        if (!tg->se)
                goto err;

        tg->shares = NICE_0_LOAD;

        init_cfs_bandwidth(tg_cfs_bandwidth(tg));

        for_each_possible_cpu(i) {     //遍历cpu， 为每个cpu创建cfs_rq和调度实体。
                cfs_rq = kzalloc_node(sizeof(struct cfs_rq),
                                      GFP_KERNEL, cpu_to_node(i));
                if (!cfs_rq)
                        goto err;

                se = kzalloc_node(sizeof(struct sched_entity),
                                  GFP_KERNEL, cpu_to_node(i));
                if (!se)
                        goto err_free_rq;

                init_cfs_rq(cfs_rq);   // 初始化cfs_rq
                init_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);   // 初始化task_group, 建立对应关系
                init_entity_runnable_average(se);
        }

        return 1;

err_free_rq:
        kfree(cfs_rq);
err:
        return 0;
}

```

## 加入调度组

cpu_cgroup_attach()函数的定义

```
static void cpu_cgroup_attach(struct cgroup_taskset *tset)
{
        struct task_struct *task;
        struct cgroup_subsys_state *css;

        cgroup_taskset_for_each(task, css, tset)
                sched_move_task(task);
}

#define cgroup_taskset_for_each(task, dst_css, tset)                    \
        for ((task) = cgroup_taskset_first((tset), &(dst_css));         \
             (task);                                                    \
             (task) = cgroup_taskset_next((tset), &(dst_css)))

```

sched_move_task() 函数迁移进程。

```
void sched_move_task(struct task_struct *tsk)
{
        int queued, running, queue_flags =
                DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
        struct rq_flags rf;
        struct rq *rq;

        rq = task_rq_lock(tsk, &rf);
        update_rq_clock(rq);

        running = task_current(rq, tsk);   // 判断当前进程是否正在运行
        queued = task_on_rq_queued(tsk);   // 判断当前进程是否在就绪队列

        if (queued)
                dequeue_task(rq, tsk, queue_flags);
        if (running)
                put_prev_task(rq, tsk);

        sched_change_group(tsk, TASK_MOVE_GROUP);

        if (queued)
                enqueue_task(rq, tsk, queue_flags);
        if (running) {
                set_next_task(rq, tsk);
                /*
                 * After changing group, the running task may have joined a
                 * throttled one but it's still the running task. Trigger a
                 * resched to make sure that task can still run.
                 */
                resched_curr(rq);
        }

        task_rq_unlock(rq, tsk, &rf);
}

```
sched_change_group函数


```
static void sched_change_group(struct task_struct *tsk, int type)
{
        struct task_group *tg;

        /*
         * All callers are synchronized by task_rq_lock(); we do not use RCU
         * which is pointless here. Thus, we pass "true" to task_css_check()
         * to prevent lockdep warnings.
         */
        tg = container_of(task_css_check(tsk, cpu_cgrp_id, true),   // 获取对应的task_cgroup
                          struct task_group, css);
        tg = autogroup_task_group(tsk, tg);
        tsk->sched_task_group = tg;

#ifdef CONFIG_QOS_SCHED         
        sched_change_qos_group(tsk, tg);
#endif

#ifdef CONFIG_BPF_SCHED
        /*
         * This function has cleared and restored the task status,
         * so we do not need to dequeue and enqueue the task again.
         */
        tsk->tag = tg->tag;
#endif

#ifdef CONFIG_FAIR_GROUP_SCHED
        if (tsk->sched_class->task_change_group)
                tsk->sched_class->task_change_group(tsk, type);   //调用调度类的task_change_group方法
        else
#endif
                set_task_rq(tsk, task_cpu(tsk));
}

```

## 调度组的使能

调度组需要使能CONFIG_FAIR_GROUP_SCHED配置。 在遍历时根据parent关系逐级遍历。

```
#define for_each_sched_entity(se) \
                for (; se; se = se->parent)

```


<br>
转载请注明： [【sched】三、CFS组调度](https://dracoding.github.io/2023/05/cfs组调度/) 

---
layout: post
title: 【sched】二、CFS调度器源码分析
date: 2023-05-26
tags:  sched 
---

## 组调度相关的结构体

组调用相关的结构体见第一章的task_cgroup

```
const struct sched_class fair_sched_class
        __section("__fair_sched_class") = {
        .enqueue_task           = enqueue_task_fair,
        .dequeue_task           = dequeue_task_fair,
        .yield_task             = yield_task_fair,
        .yield_to_task          = yield_to_task_fair,

        .check_preempt_curr     = check_preempt_wakeup,

        .pick_next_task         = __pick_next_task_fair,
        .put_prev_task          = put_prev_task_fair,
        .set_next_task          = set_next_task_fair,

        // SMP
        .balance                = balance_fair,
        .select_task_rq         = select_task_rq_fair,
        .migrate_task_rq        = migrate_task_rq_fair,
        
        .rq_online              = rq_online_fair,
        .rq_offline             = rq_offline_fair,

        .task_dead              = task_dead_fair,
        .set_cpus_allowed       = set_cpus_allowed_common,
        // end SMP
        
        .task_tick              = task_tick_fair,
        .task_fork              = task_fork_fair,
                
        .prio_changed           = prio_changed_fair,
        .switched_from          = switched_from_fair,
        .switched_to            = switched_to_fair,

        .get_rr_interval        = get_rr_interval_fair,

        .update_curr            = update_curr_fair,

        // CONFIG_FAIR_GROUP_SCHED
        .task_change_group      = task_change_group_fair,
        // end

        // #ifdef CONFIG_UCLAMP_TASK
        .uclamp_enabled         = 1,
        // #endif
};
```

## task_fork_fair函数

task_fork_fair的函数定义如下, 在fork时初始化调用

```
static void task_fork_fair(struct task_struct *p)
{
        struct cfs_rq *cfs_rq;
        struct sched_entity *se = &p->se, *curr;
        struct rq *rq = this_rq();
        struct rq_flags rf;

        rq_lock(rq, &rf);
        update_rq_clock(rq);

        cfs_rq = task_cfs_rq(current);
        curr = cfs_rq->curr;
        if (curr) {
                update_curr(cfs_rq);      // 更新正在运行的调度实体的虚拟运行时间
                se->vruntime = curr->vruntime;
        }
        place_entity(cfs_rq, se, 1);      // 根据新加入进程的权重重新计算虚拟运行时间

        if (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {
                /*
                 * Upon rescheduling, sched_class::put_prev_task() will place
                 * 'current' within the tree based on its new key value.
                 */
                swap(curr->vruntime, se->vruntime);
                resched_curr(rq);
        }

        se->vruntime -= cfs_rq->min_vruntime;
        rq_unlock(rq, &rf);
}
```

update_curr()函数的定义
```
static void update_curr(struct cfs_rq *cfs_rq)
{
        struct sched_entity *curr = cfs_rq->curr;
        u64 now = rq_clock_task(rq_of(cfs_rq));
        u64 delta_exec;

        if (unlikely(!curr))
                return;

        delta_exec = now - curr->exec_start;
        if (unlikely((s64)delta_exec <= 0))
                return;

        curr->exec_start = now;

        schedstat_set(curr->statistics.exec_max,
                      max(delta_exec, curr->statistics.exec_max));

        curr->sum_exec_runtime += delta_exec;
        schedstat_add(cfs_rq->exec_clock, delta_exec);

        curr->vruntime += calc_delta_fair(delta_exec, curr);  // 更新当前进程的虚拟运行时间
        update_min_vruntime(cfs_rq);                        //更新cfs_rq的min_vruntime

        if (entity_is_task(curr)) {
                struct task_struct *curtask = task_of(curr);

                trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
                cgroup_account_cputime(curtask, delta_exec);
                account_group_exec_runtime(curtask, delta_exec);
        }

        account_cfs_rq_runtime(cfs_rq, delta_exec);
}
```

place_entity() 函数更新调度实体的vruntime。

```
static void
place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)
{
        u64 vruntime = cfs_rq->min_vruntime;

        /*
         * The 'current' period is already promised to the current tasks,
         * however the extra weight of the new task will slow them down a
         * little, place the new task so that it fits in the slot that
         * stays open at the end.
         */
        if (initial && sched_feat(START_DEBIT))
                vruntime += sched_vslice(cfs_rq, se);  // 新创建的进程进行惩罚， 惩罚的时间由新进程的权限根据sched_vslice计算得到

        /* sleeps up to a single latency don't count. */
        if (!initial) {
                unsigned long thresh = sysctl_sched_latency;

                /*
                 * Halve their sleep time's effect, to allow
                 * for a gentler effect of sleepers:
                 */
                if (sched_feat(GENTLE_FAIR_SLEEPERS))
                        thresh >>= 1;

                vruntime -= thresh;             // 对于wakeup的进程， 更新vruntime， 提升优先级
        }

        /* ensure we never gain time by being placed backwards. */
        se->vruntime = max_vruntime(se->vruntime, vruntime);
}

```
sched_vslice函数

```
static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)  //
{
        unsigned int nr_running = cfs_rq->nr_running;
        u64 slice;

        if (sched_feat(ALT_PERIOD))
                nr_running = rq_of(cfs_rq)->cfs.h_nr_running;

        slice = __sched_period(nr_running + !se->on_rq);   // 重新计算调度队列（添加进程后）的调度时延

        for_each_sched_entity(se) {
                struct load_weight *load;
                struct load_weight lw;

                cfs_rq = cfs_rq_of(se);
                load = &cfs_rq->load;

                if (unlikely(!se->on_rq)) {
                        lw = cfs_rq->load;

                        update_load_add(&lw, se->load.weight);      // 添加当前的权重到cfs_rq中
                        load = &lw;
                }
                slice = __calc_delta(slice, se->load.weight, load); // 根据调度时延计算虚拟运行时间
        }

        if (sched_feat(BASE_SLICE))
                slice = max(slice, (u64)sysctl_sched_min_granularity);

        return slice;
}

static u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)  //根据sched_slice计算的虚拟时间得到新的虚拟时间
{
        return calc_delta_fair(sched_slice(cfs_rq, se), se);
}

```

## enqueue_task_fair函数

```
static void
enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
{
        struct cfs_rq *cfs_rq;
        struct sched_entity *se = &p->se;
        int idle_h_nr_running = task_has_idle_policy(p);
        int task_new = !(flags & ENQUEUE_WAKEUP);
        unsigned int prev_nr = rq->cfs.h_nr_running;

        for_each_sched_entity(se) {
                if (se->on_rq)
                        break;
                cfs_rq = cfs_rq_of(se);
                enqueue_entity(cfs_rq, se, flags);   //将调度实体加入到cfs_rq队列中

                cfs_rq->h_nr_running++;
                cfs_rq->idle_h_nr_running += idle_h_nr_running;

                /* end evaluation on encountering a throttled cfs_rq */
                if (cfs_rq_throttled(cfs_rq))
                        goto enqueue_throttle;

                flags = ENQUEUE_WAKEUP;
        }

       for_each_sched_entity(se) {
                cfs_rq = cfs_rq_of(se);

                update_load_avg(cfs_rq, se, UPDATE_TG);   // 更新调度实体或cfs_rq的负载
                se_update_runnable(se);
                update_cfs_group(se);

                cfs_rq->h_nr_running++;
                cfs_rq->idle_h_nr_running += idle_h_nr_running;

                /* end evaluation on encountering a throttled cfs_rq */
                if (cfs_rq_throttled(cfs_rq))
                        goto enqueue_throttle;

               /*
                * One parent has been throttled and cfs_rq removed from the
                * list. Add it back to not break the leaf list.
                */
               if (throttled_hierarchy(cfs_rq))
                       list_add_leaf_cfs_rq(cfs_rq);
        }

        /* At this point se is NULL and we are at root level*/
        add_nr_running(rq, 1);
        if (prev_nr == 1)
                overload_set(rq);

        /*
         * Since new tasks are assigned an initial util_avg equal to
         * half of the spare capacity of their CPU, tiny tasks have the
         * ability to cross the overutilized threshold, which will
         * result in the load balancer ruining all the task placement
         * done by EAS. As a way to mitigate that effect, do not account
         * for the first enqueue operation of new tasks during the
         * overutilized flag detection.
         *
         * A better way of solving this problem would be to wait for
         * the PELT signals of tasks to converge before taking them
         * into account, but that is not straightforward to implement,
         * and the following generally works well enough in practice.
         */
        if (!task_new)
                update_overutilized_status(rq);
enqueue_throttle:
        if (cfs_bandwidth_used()) {
                /*
                 * When bandwidth control is enabled; the cfs_rq_throttled()
                 * breaks in the above iteration can result in incomplete
                 * leaf list maintenance, resulting in triggering the assertion
                 * below.
                 */
                for_each_sched_entity(se) {
                        cfs_rq = cfs_rq_of(se);

                        if (list_add_leaf_cfs_rq(cfs_rq))
                                break;
                }
        }

        assert_list_leaf_cfs_rq(rq);

        hrtick_update(rq);    //更新定时器
}

```
enqueue_entity定义

```
static void
enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
{
        bool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);
        bool curr = cfs_rq->curr == se;

        /*
         * If we're the current task, we must renormalise before calling
         * update_curr().
         */
        if (renorm && curr)
                se->vruntime += cfs_rq->min_vruntime;  // fork时候减掉对应的时间， 这里从新加回

        update_curr(cfs_rq);

        /*
         * Otherwise, renormalise after, such that we're placed at the current
         * moment in time, instead of some random moment in the past. Being
         * placed in the past could significantly boost this task to the
         * fairness detriment of existing tasks.
         */
        if (renorm && !curr)
                se->vruntime += cfs_rq->min_vruntime;

        /*
         * When enqueuing a sched_entity, we must:
         *   - Update loads to have both entity and cfs_rq synced with now.
         *   - Add its load to cfs_rq->runnable_avg
         *   - For group_entity, update its weight to reflect the new share of
         *     its group cfs_rq
         *   - Add its new weight to cfs_rq->load.weight
         */
        update_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);
        se_update_runnable(se);
        update_cfs_group(se);
        account_entity_enqueue(cfs_rq, se);

        if (flags & ENQUEUE_WAKEUP)
                place_entity(cfs_rq, se, 0);

        check_schedstat_required();
        update_stats_enqueue(cfs_rq, se, flags);
        check_spread(cfs_rq, se);
        if (!curr)
#ifdef CONFIG_DTS
                if (se->by_pass != INIT_BY_PASS)
#endif
                        __enqueue_entity(cfs_rq, se);  // 调度实体加入cfs_rq队列

        se->on_rq = 1;
```


## __pick_next_task_fair函数

```
static struct task_struct *__pick_next_task_fair(struct rq *rq)
{
        return pick_next_task_fair(rq, NULL, NULL);
}
```

简化的pick_next_task_fair函数定义

```
struct task_struct *
pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
        struct cfs_rq *cfs_rq = &rq->cfs;
        struct sched_entity *se;
        struct task_struct *p;
        int new_tasks;
        unsigned long time;

again:


        if (!sched_fair_runnable(rq))
                goto idle;

#ifdef CONFIG_FAIR_GROUP_SCHED
        if (!prev || prev->sched_class != &fair_sched_class) {
                goto simple;
        }

simple:
#endif
        if (prev)
                put_prev_task(rq, prev);   // 调用调度类的方法put_prev_task

        do {
                se = pick_next_entity(cfs_rq, NULL);   //选择下一个调度实体
                set_next_entity(cfs_rq, se);   // cfs_rq中删除调度实体并设置为cfs_rq->curr
                cfs_rq = group_cfs_rq(se);
        } while (cfs_rq);          // cfs_rq为se的my_q对象， 组调度时为true

        p = task_of(se);

done: __maybe_unused;
#ifdef CONFIG_SMP
        /*
         * Move the next running task to the front of
         * the list, so our cfs_tasks list becomes MRU
         * one.
         */
#ifdef CONFIG_SCHED_PRIO_LB
        adjust_rq_cfs_tasks(list_move, rq, &p->se);
#else
        list_move(&p->se.group_node, &rq->cfs_tasks);
#endif
#endif

        if (hrtick_enabled(rq))
                hrtick_start_fair(rq, p);

        update_misfit_status(p, rq);

        return p;
idle:
        if (!rf)
                return NULL;

        time = schedstat_start_time();

        /*
         * We must set idle_stamp _before_ calling try_steal() or
         * idle_balance(), such that we measure the duration as idle time.
         */
        rq_idle_stamp_update(rq);

        new_tasks = newidle_balance(rq, rf);
        if (new_tasks == 0)
                new_tasks = try_steal(rq, rf);
        schedstat_end_time(rq, time);

        if (new_tasks)
                rq_idle_stamp_clear(rq);

        if (new_tasks < 0)
                return RETRY_TASK;

        if (new_tasks > 0)
                goto again;

        /*
         * rq is about to be idle, check if we need to update the
         * lost_idle_time of clock_pelt
         */
        update_idle_rq_clock_pelt(rq);

        return NULL;
}                  

```

## task_change_group_fair函数

```
static void task_change_group_fair(struct task_struct *p, int type)
{       
        switch (type) {
        case TASK_SET_GROUP:
                task_set_group_fair(p);
                break;

        case TASK_MOVE_GROUP:
                task_move_group_fair(p);
                break;
        }
}

```

task_move_group_fair

```
static void task_move_group_fair(struct task_struct *p)
{
        detach_task_cfs_rq(p);
        set_task_rq(p, task_cpu(p));     // 设置进程调度实体的cfs_rq和parent

#ifdef CONFIG_SMP
        /* Tell se's cfs_rq has been changed -- migrated */
        p->se.avg.last_update_time = 0;
#endif    
        attach_task_cfs_rq(p);   // 加入task后重新计算负载， 权重等信息，
}       

```

## task_tick_fair函数

```
static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
{
        struct cfs_rq *cfs_rq;
        struct sched_entity *se = &curr->se;

#ifdef CONFIG_DTS
        if (curr->by_pass != NONE_BY_PASS) {
                se = &curr->dts_shared_se;
        }
#endif 

        for_each_sched_entity(se) {
                cfs_rq = cfs_rq_of(se);
                entity_tick(cfs_rq, se, queued);   // 负载的重新计算
        }

        if (static_branch_unlikely(&sched_numa_balancing))
                task_tick_numa(rq, curr);     // 执行numa相关的操作， 内存的迁移

        update_misfit_status(curr, rq);
        update_overutilized_status(task_rq(curr));
}
```

## select_task_rq_fair

该函数选择对应的队列进行任务的放置

```
static int
select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)
{
        unsigned long time;
        struct sched_domain *tmp, *sd = NULL;
        int cpu = smp_processor_id();
        int new_cpu = prev_cpu;
        int want_affine = 0;
        int sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);

        if ((wake_flags & WF_CURRENT_CPU) && cpumask_test_cpu(cpu, p->cpus_ptr))
                return cpu;


        time = schedstat_start_time();

        if (sd_flag & SD_BALANCE_WAKE) {
                record_wakee(p);      // 记录wake相关的信息

                if (sched_energy_enabled()) {
                        new_cpu = find_energy_efficient_cpu(p, prev_cpu);
                        if (new_cpu >= 0)
                                return new_cpu;
                        new_cpu = prev_cpu;
                }

                want_affine = !wake_wide(p) && cpumask_test_cpu(cpu, p->cpus_ptr);   // 只有有唤醒关系的进程才可能存在数据共享
        }

        rcu_read_lock();
        for_each_domain(cpu, tmp) {               // 当前cpu所属的sched_domain向上遍历， 选中sched_domain
                /*
                 * If both 'cpu' and 'prev_cpu' are part of this domain,
                 * cpu is a valid SD_WAKE_AFFINE target.
                 */
                if (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&
                    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {
                        if (cpu != prev_cpu)
                                new_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);

                        sd = NULL; /* Prefer wake_affine over balance flags */
                        break;
                }

                if (tmp->flags & sd_flag)
                        sd = tmp; 
                else if (!want_affine)
                        break;
        }

        if (unlikely(sd)) {
                /* Slow path */
                new_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);    // 在选中的sched_domain中选择空闲的cpu
        } else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */
                /* Fast path */

                new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);   // 根据llc domain选择距离new_cpu较近的空闲CPU

                if (want_affine)
                        current->recent_used_cpu = cpu;
        }
        rcu_read_unlock();
        schedstat_end_time(cpu_rq(cpu), time);

        return new_cpu;
}
```

<br>
转载请注明： [【sched】二、CFS调度器源码分析l](https://dracoding.github.io/2023/05/cfs调度器源码分析/) 